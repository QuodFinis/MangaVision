{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"../Manga109/annotations\"\n",
    "output_path = \"clean_manga109\"\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "def parse_xml_file(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # init containers\n",
    "    manga_data = {\n",
    "        \"title\": root.get(\"title\"),\n",
    "        \"characters\": {},\n",
    "        \"pages\": []\n",
    "    }\n",
    "\n",
    "    # extract character id and names\n",
    "    for character in root.find(\"characters\"):\n",
    "        character_id = character.get(\"id\")\n",
    "        character_name = character.get('name')\n",
    "        manga_data[\"characters\"][character_id] = character_name\n",
    "\n",
    "    # extract pages and objects\n",
    "    for page in root.find(\"pages\"):\n",
    "        page_data = {\n",
    "            \"page_number\": int(page.get(\"index\")),\n",
    "            \"width\": int(page.get(\"width\")),\n",
    "            \"height\": int(page.get(\"height\")),\n",
    "            \"objects\": []\n",
    "        }\n",
    "\n",
    "        for obj in page:\n",
    "            # extract id and bounding box coordinates\n",
    "            obj_data = {\n",
    "                \"id\": obj.get(\"id\"),\n",
    "                \"type\": obj.tag,\n",
    "                \"xmin\": int(obj.get(\"xmin\")),\n",
    "                \"ymin\": int(obj.get(\"ymin\")),\n",
    "                \"xmax\": int(obj.get(\"xmax\")),\n",
    "                \"ymax\": int(obj.get(\"ymax\"))\n",
    "            }\n",
    "\n",
    "            # extract character ID and name, then associate them to face and body\n",
    "            # extract text\n",
    "            if obj.tag in [\"face\", \"body\"]:\n",
    "                obj_data[\"character_id\"] = obj.get(\"character\")\n",
    "                obj_data[\"character_name\"] = manga_data[\"characters\"].get(obj_data[\"character_id\"], \"Unknown\")\n",
    "            elif obj.tag == \"text\":\n",
    "                obj_data[\"text_content\"] = obj.text.strip() if obj.text else \"\"\n",
    "            \n",
    "            page_data[\"objects\"].append(obj_data)\n",
    "\n",
    "        manga_data[\"pages\"].append(page_data)\n",
    "\n",
    "    return manga_data\n",
    "\n",
    "\n",
    "def clean_dataset(manga_data, output_dir):\n",
    "    title = manga_data[\"title\"]\n",
    "    for page in manga_data[\"pages\"]:\n",
    "        page_df = pd.DataFrame(page[\"objects\"])\n",
    "        page_df[\"page_number\"] = page[\"page_number\"]\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{title}_{page['page_number']:03}.csv\")\n",
    "        page_df.to_csv(output_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned: AisazuNihaIrarenai\n",
      "Data cleaned: AkkeraKanjinchou\n",
      "Data cleaned: Akuhamu\n",
      "Data cleaned: AosugiruHaru\n",
      "Data cleaned: AppareKappore\n",
      "Data cleaned: Arisa\n",
      "Data cleaned: ARMS\n",
      "Data cleaned: BakuretsuKungFuGirl\n",
      "Data cleaned: Belmondo\n",
      "Data cleaned: BEMADER_P\n",
      "Data cleaned: BokuHaSitatakaKun\n",
      "Data cleaned: BurariTessenTorimonocho\n",
      "Data cleaned: ByebyeC-BOY\n",
      "Data cleaned: AisazuNihaIrarenai\n",
      "Data cleaned: Arisa\n",
      "Data cleaned: Count3DeKimeteAgeru\n",
      "Data cleaned: DollGun\n",
      "Data cleaned: Donburakokko\n",
      "Data cleaned: DualJustice\n",
      "Data cleaned: EienNoWith\n",
      "Data cleaned: EvaLady\n",
      "Data cleaned: EverydayOsakanaChan\n",
      "Data cleaned: GakuenNoise\n",
      "Data cleaned: GarakutayaManta\n",
      "Data cleaned: GinNoChimera\n",
      "Data cleaned: GOOD_KISS_Ver2\n",
      "Data cleaned: Hamlet\n",
      "Data cleaned: HanzaiKousyouninMinegishiEitarou\n",
      "Data cleaned: HaruichibanNoFukukoro\n",
      "Data cleaned: HarukaRefrain\n",
      "Data cleaned: HealingPlanet\n",
      "Data cleaned: HeiseiJimen\n",
      "Data cleaned: HighschoolKimengumi_vol01\n",
      "Data cleaned: HighschoolKimengumi_vol20\n",
      "Data cleaned: HinagikuKenzan\n",
      "Data cleaned: HisokaReturns\n",
      "Data cleaned: JangiriPonpon\n",
      "Data cleaned: JijiBabaFight\n",
      "Data cleaned: Joouari\n",
      "Data cleaned: Jyovolley\n",
      "Data cleaned: KarappoHighschool\n",
      "Data cleaned: KimiHaBokuNoTaiyouDa\n",
      "Data cleaned: KoukouNoHitotachi\n",
      "Data cleaned: KuroidoGanka\n",
      "Data cleaned: KyokugenCyclone\n",
      "Data cleaned: LancelotFullThrottle\n",
      "Data cleaned: LoveHina_vol01\n",
      "Data cleaned: LoveHina_vol14\n",
      "Data cleaned: MadouTaiga\n",
      "Data cleaned: MAD_STONE\n",
      "Data cleaned: MagicianLoad\n",
      "Data cleaned: MagicStarGakuin\n",
      "Data cleaned: MariaSamaNihaNaisyo\n",
      "Data cleaned: MayaNoAkaiKutsu\n",
      "Data cleaned: MemorySeijin\n",
      "Data cleaned: MeteoSanStrikeDesu\n",
      "Data cleaned: MiraiSan\n",
      "Data cleaned: MisutenaideDaisy\n",
      "Data cleaned: MoeruOnisan_vol01\n",
      "Data cleaned: MoeruOnisan_vol19\n",
      "Data cleaned: MomoyamaHaikagura\n",
      "Data cleaned: MukoukizuNoChonbo\n",
      "Data cleaned: MutekiBoukenSyakuma\n",
      "Data cleaned: Nekodama\n",
      "Data cleaned: NichijouSoup\n",
      "Data cleaned: Ningyoushi\n",
      "Data cleaned: OhWareraRettouSeitokai\n",
      "Data cleaned: OL_Lunch\n",
      "Data cleaned: ParaisoRoad\n",
      "Data cleaned: PikaruGenkiDesu\n",
      "Data cleaned: PLANET7\n",
      "Data cleaned: PlatinumJungle\n",
      "Data cleaned: PrayerHaNemurenai\n",
      "Data cleaned: PrismHeart\n",
      "Data cleaned: PsychoStaff\n",
      "Data cleaned: Raphael\n",
      "Data cleaned: ReveryEarth\n",
      "Data cleaned: RinToSiteSippuNoNaka\n",
      "Data cleaned: RisingGirl\n",
      "Data cleaned: Saisoku\n",
      "Data cleaned: SaladDays_vol01\n",
      "Data cleaned: SaladDays_vol18\n",
      "Data cleaned: SamayoeruSyonenNiJunaiWo\n",
      "Data cleaned: SeisinkiVulnus\n",
      "Data cleaned: ShimatteIkouze_vol01\n",
      "Data cleaned: ShimatteIkouze_vol26\n",
      "Data cleaned: SonokiDeABC\n",
      "Data cleaned: SyabondamaKieta\n",
      "Data cleaned: TaiyouNiSmash\n",
      "Data cleaned: TapkunNoTanteisitsu\n",
      "Data cleaned: TasogareTsushin\n",
      "Data cleaned: TennenSenshiG\n",
      "Data cleaned: TensiNoHaneToAkumaNoShippo\n",
      "Data cleaned: TetsuSan\n",
      "Data cleaned: That'sIzumiko\n",
      "Data cleaned: TotteokiNoABC\n",
      "Data cleaned: ToutaMairimasu\n",
      "Data cleaned: TouyouKidan\n",
      "Data cleaned: TsubasaNoKioku\n",
      "Data cleaned: UchiNoNyan'sDiary\n",
      "Data cleaned: UchuKigekiM774\n",
      "Data cleaned: UltraEleven\n",
      "Data cleaned: UnbalanceTokyo\n",
      "Data cleaned: WarewareHaOniDearu\n",
      "Data cleaned: YamatoNoHane\n",
      "Data cleaned: YasasiiAkuma\n",
      "Data cleaned: YouchienBoueigumi\n",
      "Data cleaned: YoumaKourin\n",
      "Data cleaned: YukiNoFuruMachi\n",
      "Data cleaned: YumeiroCooking\n",
      "Data cleaned: YumeNoKayoiji\n"
     ]
    }
   ],
   "source": [
    "xml_files = [f for f in os.listdir(data_path) if f.endswith('.xml')]\n",
    "\n",
    "for title in xml_files:\n",
    "  manga_title = title[:-4]\n",
    "\n",
    "  xml_file = os.path.join(data_path, f'{manga_title}.xml')\n",
    "\n",
    "  manga_data = parse_xml_file(xml_file)\n",
    "\n",
    "  output_path_manga = os.path.join(output_path, manga_title)\n",
    "  os.makedirs(output_path_manga, exist_ok=True)\n",
    "\n",
    "  clean_dataset(manga_data, output_path_manga)\n",
    "\n",
    "  print(f\"Data cleaned: {manga_data['title']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of panels found: 12\n",
      "Panel 1 has 2 face(s)\n",
      "Panel 2 has 1 face(s)\n",
      "Panel 3 has 0 face(s)\n",
      "Panel 4 has 0 face(s)\n",
      "Panel 5 has 0 face(s)\n",
      "Panel 6 has 0 face(s)\n",
      "Panel 7 has 1 face(s)\n",
      "Panel 8 has 1 face(s)\n",
      "Panel 9 has 0 face(s)\n",
      "Panel 10 has 0 face(s)\n",
      "Panel 11 has 0 face(s)\n",
      "Panel 12 has 1 face(s)\n"
     ]
    }
   ],
   "source": [
    "def count_faces_in_panels(csv_file_path):\n",
    "    # Load the DataFrame from the CSV file\n",
    "    try:\n",
    "        page_df = pd.read_csv(csv_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Debugging: Print the columns and the first few rows\n",
    "    # print(\"Columns in page_df:\", page_df.columns)\n",
    "    # print(page_df.head())\n",
    "\n",
    "    # Filter to get only panels\n",
    "    panels = page_df[page_df[\"type\"] == \"frame\"]  \n",
    "\n",
    "    print(f\"Total number of panels found: {len(panels)}\")  # Print total panels\n",
    "\n",
    "    # Use enumerate for proper indexing\n",
    "    for index, panel in enumerate(panels.iterrows()):  \n",
    "        panel_index = index + 1  # Make it 1-based index\n",
    "        # Get the bounding box of the panel\n",
    "        xmin, ymin, xmax, ymax = panel[1][\"xmin\"], panel[1][\"ymin\"], panel[1][\"xmax\"], panel[1][\"ymax\"]\n",
    "\n",
    "        # Count the number of faces inside this panel's bounding box\n",
    "        faces_in_panel = page_df[\n",
    "            (page_df[\"type\"] == \"face\") &\n",
    "            (page_df[\"xmin\"] >= xmin) &\n",
    "            (page_df[\"xmax\"] <= xmax) &\n",
    "            (page_df[\"ymin\"] >= ymin) &\n",
    "            (page_df[\"ymax\"] <= ymax)\n",
    "        ].shape[0]\n",
    "\n",
    "        print(f\"Panel {panel_index} has {faces_in_panel} face(s)\")  \n",
    "\n",
    "# Input usage:\n",
    "csv_file_path = \"clean_manga109\\AisazuNihaIrarenai\\AisazuNihaIrarenai_002.csv\"  # Replace with the path to the CSV file\n",
    "count_faces_in_panels(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manga_title | total_panel | total_faces | avg_num_faces_per_panel | total_text | avg_num_text_per_panel\n",
      "AisazuNihaIrarenai | 1014 | 1139 | 1.12 | 1473 | 1.45\n",
      "AkkeraKanjinchou | 975 | 736 | 0.75 | 1340 | 1.37\n",
      "Akuhamu | 873 | 1207 | 1.38 | 1461 | 1.67\n",
      "AosugiruHaru | 964 | 632 | 0.66 | 1360 | 1.41\n",
      "AppareKappore | 772 | 930 | 1.20 | 1246 | 1.61\n",
      "Arisa | 910 | 1228 | 1.35 | 1391 | 1.53\n",
      "ARMS | 575 | 327 | 0.57 | 1062 | 1.85\n",
      "BakuretsuKungFuGirl | 1010 | 1104 | 1.09 | 2189 | 2.17\n",
      "Belmondo | 921 | 1000 | 1.09 | 1347 | 1.46\n",
      "BEMADER_P | 1141 | 1354 | 1.19 | 1449 | 1.27\n",
      "BokuHaSitatakaKun | 1065 | 2526 | 2.37 | 2255 | 2.12\n",
      "BurariTessenTorimonocho | 1335 | 1235 | 0.93 | 1653 | 1.24\n",
      "ByebyeC-BOY | 1125 | 1035 | 0.92 | 1439 | 1.28\n",
      "Copy of AisazuNihaIrarenai | 1014 | 1139 | 1.12 | 1473 | 1.45\n",
      "Copy of Arisa | 910 | 1228 | 1.35 | 1391 | 1.53\n",
      "Count3DeKimeteAgeru | 947 | 784 | 0.83 | 1228 | 1.30\n",
      "DollGun | 865 | 1155 | 1.34 | 1289 | 1.49\n",
      "Donburakokko | 605 | 582 | 0.96 | 935 | 1.55\n",
      "DualJustice | 783 | 440 | 0.56 | 868 | 1.11\n",
      "EienNoWith | 1385 | 1160 | 0.84 | 1752 | 1.26\n",
      "EvaLady | 991 | 887 | 0.90 | 1506 | 1.52\n",
      "EverydayOsakanaChan | 911 | 544 | 0.60 | 1904 | 2.09\n",
      "GakuenNoise | 1076 | 951 | 0.88 | 1350 | 1.25\n",
      "GarakutayaManta | 1168 | 1287 | 1.10 | 1147 | 0.98\n",
      "GinNoChimera | 751 | 701 | 0.93 | 824 | 1.10\n",
      "GOOD_KISS_Ver2 | 1104 | 1147 | 1.04 | 1548 | 1.40\n",
      "Hamlet | 1626 | 2570 | 1.58 | 2748 | 1.69\n",
      "HanzaiKousyouninMinegishiEitarou | 1207 | 1036 | 0.86 | 1715 | 1.42\n",
      "HaruichibanNoFukukoro | 955 | 675 | 0.71 | 1468 | 1.54\n",
      "HarukaRefrain | 839 | 1009 | 1.20 | 1148 | 1.37\n",
      "HealingPlanet | 791 | 573 | 0.72 | 997 | 1.26\n",
      "HeiseiJimen | 1008 | 1486 | 1.47 | 1202 | 1.19\n",
      "HighschoolKimengumi_vol01 | 987 | 1856 | 1.88 | 1863 | 1.89\n",
      "HighschoolKimengumi_vol20 | 960 | 1609 | 1.68 | 1826 | 1.90\n",
      "HinagikuKenzan | 817 | 655 | 0.80 | 940 | 1.15\n",
      "HisokaReturns | 984 | 878 | 0.89 | 1163 | 1.18\n",
      "JangiriPonpon | 1445 | 1820 | 1.26 | 1351 | 0.93\n",
      "JijiBabaFight | 862 | 1272 | 1.48 | 1541 | 1.79\n",
      "Joouari | 952 | 950 | 1.00 | 1167 | 1.23\n",
      "Jyovolley | 781 | 777 | 0.99 | 889 | 1.14\n",
      "KarappoHighschool | 1364 | 1288 | 0.94 | 1274 | 0.93\n",
      "KimiHaBokuNoTaiyouDa | 1053 | 953 | 0.91 | 1396 | 1.33\n",
      "KoukouNoHitotachi | 938 | 2133 | 2.27 | 2423 | 2.58\n",
      "KuroidoGanka | 1029 | 716 | 0.70 | 1226 | 1.19\n",
      "KyokugenCyclone | 489 | 789 | 1.61 | 768 | 1.57\n",
      "LancelotFullThrottle | 942 | 1359 | 1.44 | 1672 | 1.77\n",
      "LoveHina_vol01 | 1077 | 1305 | 1.21 | 1810 | 1.68\n",
      "LoveHina_vol14 | 1015 | 1551 | 1.53 | 1895 | 1.87\n",
      "MadouTaiga | 670 | 468 | 0.70 | 716 | 1.07\n",
      "MAD_STONE | 619 | 700 | 1.13 | 605 | 0.98\n",
      "MagicianLoad | 754 | 668 | 0.89 | 907 | 1.20\n",
      "MagicStarGakuin | 659 | 917 | 1.39 | 1376 | 2.09\n",
      "MariaSamaNihaNaisyo | 975 | 724 | 0.74 | 1485 | 1.52\n",
      "MayaNoAkaiKutsu | 804 | 627 | 0.78 | 1046 | 1.30\n",
      "MemorySeijin | 932 | 743 | 0.80 | 1184 | 1.27\n",
      "MeteoSanStrikeDesu | 877 | 1155 | 1.32 | 1169 | 1.33\n",
      "MiraiSan | 1017 | 1428 | 1.40 | 1380 | 1.36\n",
      "MisutenaideDaisy | 1143 | 1853 | 1.62 | 2051 | 1.79\n",
      "MoeruOnisan_vol01 | 908 | 2114 | 2.33 | 2167 | 2.39\n",
      "MoeruOnisan_vol19 | 1112 | 2649 | 2.38 | 2245 | 2.02\n",
      "MomoyamaHaikagura | 491 | 572 | 1.16 | 710 | 1.45\n",
      "MukoukizuNoChonbo | 606 | 977 | 1.61 | 1065 | 1.76\n",
      "MutekiBoukenSyakuma | 875 | 963 | 1.10 | 1227 | 1.40\n",
      "Nekodama | 828 | 1327 | 1.60 | 1351 | 1.63\n",
      "NichijouSoup | 845 | 1184 | 1.40 | 1665 | 1.97\n",
      "Ningyoushi | 922 | 822 | 0.89 | 963 | 1.04\n",
      "OhWareraRettouSeitokai | 1115 | 1035 | 0.93 | 1330 | 1.19\n",
      "OL_Lunch | 831 | 1481 | 1.78 | 1808 | 2.18\n",
      "ParaisoRoad | 904 | 1166 | 1.29 | 1435 | 1.59\n",
      "PikaruGenkiDesu | 1040 | 1406 | 1.35 | 1034 | 0.99\n",
      "PLANET7 | 850 | 1256 | 1.48 | 1548 | 1.82\n",
      "PlatinumJungle | 747 | 607 | 0.81 | 921 | 1.23\n",
      "PrayerHaNemurenai | 752 | 864 | 1.15 | 1181 | 1.57\n",
      "PrismHeart | 823 | 1001 | 1.22 | 1407 | 1.71\n",
      "PsychoStaff | 857 | 754 | 0.88 | 1115 | 1.30\n",
      "Raphael | 962 | 1162 | 1.21 | 1704 | 1.77\n",
      "ReveryEarth | 983 | 921 | 0.94 | 1280 | 1.30\n",
      "RinToSiteSippuNoNaka | 985 | 1345 | 1.37 | 1547 | 1.57\n",
      "RisingGirl | 1100 | 1193 | 1.08 | 1596 | 1.45\n",
      "Saisoku | 920 | 661 | 0.72 | 1164 | 1.27\n",
      "SaladDays_vol01 | 1024 | 846 | 0.83 | 1323 | 1.29\n",
      "SaladDays_vol18 | 882 | 687 | 0.78 | 1288 | 1.46\n",
      "SamayoeruSyonenNiJunaiWo | 774 | 738 | 0.95 | 852 | 1.10\n",
      "SeisinkiVulnus | 942 | 754 | 0.80 | 955 | 1.01\n",
      "ShimatteIkouze_vol01 | 1350 | 2231 | 1.65 | 1753 | 1.30\n",
      "ShimatteIkouze_vol26 | 1266 | 2356 | 1.86 | 1453 | 1.15\n",
      "SonokiDeABC | 925 | 1079 | 1.17 | 1164 | 1.26\n",
      "SyabondamaKieta | 894 | 929 | 1.04 | 1079 | 1.21\n",
      "TaiyouNiSmash | 767 | 688 | 0.90 | 920 | 1.20\n",
      "TapkunNoTanteisitsu | 1204 | 1007 | 0.84 | 1337 | 1.11\n",
      "TasogareTsushin | 912 | 844 | 0.93 | 1175 | 1.29\n",
      "TennenSenshiG | 975 | 662 | 0.68 | 1541 | 1.58\n",
      "TensiNoHaneToAkumaNoShippo | 845 | 934 | 1.11 | 1144 | 1.35\n",
      "TetsuSan | 715 | 1021 | 1.43 | 1179 | 1.65\n",
      "That'sIzumiko | 1127 | 946 | 0.84 | 1364 | 1.21\n",
      "TotteokiNoABC | 1110 | 1066 | 0.96 | 1534 | 1.38\n",
      "ToutaMairimasu | 891 | 1358 | 1.52 | 987 | 1.11\n",
      "TouyouKidan | 1496 | 1544 | 1.03 | 1394 | 0.93\n",
      "TsubasaNoKioku | 1179 | 445 | 0.38 | 1000 | 0.85\n",
      "UchiNoNyan'sDiary | 985 | 424 | 0.43 | 1213 | 1.23\n",
      "UchuKigekiM774 | 1145 | 1585 | 1.38 | 1813 | 1.58\n",
      "UltraEleven | 1214 | 1648 | 1.36 | 1458 | 1.20\n",
      "UnbalanceTokyo | 956 | 1175 | 1.23 | 1347 | 1.41\n",
      "WarewareHaOniDearu | 965 | 990 | 1.03 | 1150 | 1.19\n",
      "YamatoNoHane | 1029 | 1023 | 0.99 | 1016 | 0.99\n",
      "YasasiiAkuma | 836 | 957 | 1.14 | 1590 | 1.90\n",
      "YouchienBoueigumi | 368 | 737 | 2.00 | 851 | 2.31\n",
      "YoumaKourin | 950 | 985 | 1.04 | 1196 | 1.26\n",
      "YukiNoFuruMachi | 949 | 1117 | 1.18 | 1665 | 1.75\n",
      "YumeiroCooking | 1063 | 1208 | 1.14 | 1354 | 1.27\n",
      "YumeNoKayoiji | 819 | 515 | 0.63 | 912 | 1.11\n"
     ]
    }
   ],
   "source": [
    "def calculate_avg_faces_and_texts_across_manga(base_dir):\n",
    "    # Dictionary to hold results for each manga\n",
    "    results = {}\n",
    "\n",
    "    # Iterate through each folder in the base directory\n",
    "    for manga_folder in os.listdir(base_dir):\n",
    "        manga_path = os.path.join(base_dir, manga_folder)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(manga_path):\n",
    "            total_panels = 0\n",
    "            total_faces = 0\n",
    "            total_texts = 0\n",
    "\n",
    "            # Iterate through each CSV file in the manga folder\n",
    "            for csv_file in os.listdir(manga_path):\n",
    "                if csv_file.endswith('.csv'):\n",
    "                    csv_file_path = os.path.join(manga_path, csv_file)\n",
    "\n",
    "                    # Load the DataFrame from the CSV file\n",
    "                    try:\n",
    "                        page_df = pd.read_csv(csv_file_path)\n",
    "                        \n",
    "                        # Check if 'type' column exists before proceeding\n",
    "                        if 'type' not in page_df.columns:\n",
    "                            # print(f\"The 'type' column is missing in {csv_file_path}.\")\n",
    "                            continue\n",
    "\n",
    "                        # Count total panels (frames), faces, and texts\n",
    "                        total_panels += page_df[page_df[\"type\"] == \"frame\"].shape[0]\n",
    "                        total_faces += page_df[page_df[\"type\"] == \"face\"].shape[0]\n",
    "                        total_texts += page_df[page_df[\"type\"] == \"text\"].shape[0]\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {csv_file_path}: {e}\")\n",
    "\n",
    "            # Calculate average number of faces and texts per panel for the current manga\n",
    "            avg_num_faces_per_panel = total_faces / total_panels if total_panels > 0 else 0\n",
    "            avg_num_texts_per_panel = total_texts / total_panels if total_panels > 0 else 0\n",
    "            \n",
    "            # Store results for the current manga\n",
    "            results[manga_folder] = {\n",
    "                \"total_panels\": total_panels,\n",
    "                \"total_faces\": total_faces,\n",
    "                \"total_texts\": total_texts,\n",
    "                \"avg_num_faces_per_panel\": avg_num_faces_per_panel,\n",
    "                \"avg_num_texts_per_panel\": avg_num_texts_per_panel\n",
    "            }\n",
    "\n",
    "    # Print results in the desired format\n",
    "    print(\"manga_title | total_panel | total_faces | avg_num_faces_per_panel | total_text | avg_num_text_per_panel\")\n",
    "    for manga_title, data in results.items():\n",
    "        print(f\"{manga_title} | {data['total_panels']} | {data['total_faces']} | {data['avg_num_faces_per_panel']:.2f} | {data['total_texts']} | {data['avg_num_texts_per_panel']:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "calculate_avg_faces_and_texts_across_manga('clean_manga109')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
